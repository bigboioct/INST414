{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['Drama', 'Comedy', 'Horror']\n",
    "\n",
    "def calculate_metrics(filename):\n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            actual_genres = eval(row['actual genres'])\n",
    "            for genre in genres:\n",
    "                correct = row['correct?']\n",
    "                \n",
    "                if genre in actual_genres:\n",
    "                    results['positive_'+genre].append(int(correct))\n",
    "                else:\n",
    "                    results['negative_'+genre].append(int(correct))\n",
    "                    \n",
    "                if row['predicted'] == genre:\n",
    "                    results['predicted_'+genre].append(int(correct))\n",
    "                    \n",
    "    for genre in genres:\n",
    "        tp = sum(results['positive_'+genre])\n",
    "        fp = len(results['predicted_'+genre]) - tp\n",
    "        fn = sum(results['positive_'+genre]) - tp\n",
    "        tn = len(results['negative_'+genre]) - fp\n",
    "        \n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision and recall) else 0\n",
    "        \n",
    "        print(f'{genre} Accuracy: {accuracy:.3f}')\n",
    "        print(f'{genre} Precision: {precision:.3f}') \n",
    "        print(f'{genre} Recall: {recall:.3f}')\n",
    "        print(f'{genre} F1: {f1:.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1:\n",
      "Drama Accuracy: 1.000\n",
      "Drama Precision: 1.000\n",
      "Drama Recall: 1.000\n",
      "Drama F1: 1.000\n",
      "\n",
      "Comedy Accuracy: 0.984\n",
      "Comedy Precision: 0.681\n",
      "Comedy Recall: 1.000\n",
      "Comedy F1: 0.810\n",
      "\n",
      "Horror Accuracy: 0.964\n",
      "Horror Precision: 0.313\n",
      "Horror Recall: 1.000\n",
      "Horror F1: 0.477\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Model 1:')\n",
    "calculate_metrics('../data/prediction_model_01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2:\n",
      "Drama Accuracy: 0.837\n",
      "Drama Precision: 0.730\n",
      "Drama Recall: 1.000\n",
      "Drama F1: 0.844\n",
      "\n",
      "Comedy Accuracy: 1.031\n",
      "Comedy Precision: 1.167\n",
      "Comedy Recall: 1.000\n",
      "Comedy F1: 1.077\n",
      "\n",
      "Horror Accuracy: 0.994\n",
      "Horror Precision: 0.932\n",
      "Horror Recall: 1.000\n",
      "Horror F1: 0.965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Model 2:')\n",
    "calculate_metrics('../data/prediction_model_02.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3:\n",
      "Drama Accuracy: 0.498\n",
      "Drama Precision: 0.498\n",
      "Drama Recall: 1.000\n",
      "Drama F1: 0.665\n",
      "\n",
      "Comedy Accuracy: 1.155\n",
      "Comedy Precision: 0.000\n",
      "Comedy Recall: 1.000\n",
      "Comedy F1: 0.000\n",
      "\n",
      "Horror Accuracy: 1.032\n",
      "Horror Precision: 0.000\n",
      "Horror Recall: 1.000\n",
      "Horror F1: 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Model 3:')\n",
    "calculate_metrics('../data/prediction_model_03.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analyzing the accuracy, precision, recall and F1 scores of Models 1, 2, and 3, it is evident that Model 1 produces the best genre predictions overall.\n",
    "\n",
    "For Drama, Model 1 achieved a perfect score in accuracy, precision, recall and F1, which means that all the predictions made were accurate with no false positives or negatives.\n",
    "\n",
    "For Comedy, Model 1 had a decent accuracy of 0.984 and the best F1 score of 0.810. In contrast, Model 2 had lower precision and was over-predicting comedy, resulting in more false positives.\n",
    "\n",
    "For Horror, Model 1 again had the best F1 score of 0.477, but with a lower precision. However, the recall was perfect, which means that all actual horrors were correctly predicted. On the other hand, Model 2 had higher recall but lower precision and F1 scores, making it less accurate for comedies.\n",
    "\n",
    "Model 3 had poor accuracy and F1 scores of 0 for Comedy and Horror, which means it failed to predict those genres at all."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
